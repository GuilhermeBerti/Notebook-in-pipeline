# 📌 Projeto: Uso de Notebooks do Spark em Pipelines de Dados no Azure
## 📖 Descrição

Este projeto demonstra como integrar e executar notebooks do Apache Spark dentro de pipelines do Azure Synapse Analytics, aplicando conceitos de Engenharia de Dados em escala corporativa.
O objetivo principal é automatizar o processamento de dados distribuídos, utilizando Azure Synapse Pipelines para orquestrar tarefas de ingestão, transformação e análise em notebooks Spark.

## 🔑 Principais Funcionalidades

Integração com Azure Synapse Analytics: execução de notebooks Spark diretamente em pipelines.
Orquestração de tarefas: uso de atividades de notebook no pipeline para garantir automação e escalabilidade.
Transformação de dados em larga escala: processamento distribuído com Spark, aproveitando clusters de pools Spark no Synapse.
Gerenciamento de dependências: controle de variáveis e parâmetros entre células e atividades do pipeline.
Armazenamento otimizado: uso de formatos como Parquet e Delta Lake para alta performance em consultas analíticas.

## ⚙️ Tecnologias Utilizadas

Azure Synapse Analytics (Pipelines + Pools Spark)
Apache Spark
Parquet / Delta Lake
Python / PySpark

## 🎯 Objetivo do Projeto

Exemplificar uma abordagem prática e escalável para engenharia de dados em nuvem, permitindo que pipelines do Synapse orquestrem a execução de notebooks Spark de forma automatizada e integrada, reduzindo esforço manual e acelerando processos de análise.
